{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himavamsianumula/Cart-Pole/blob/master/cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gf8O-uqHfz3",
        "colab_type": "text"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dujlcL1XHfz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as K\n",
        "import gym\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCEpfDuEHfz7",
        "colab_type": "text"
      },
      "source": [
        "Testing with random actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLt19yHdHfz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env=gym.make(\"CartPole-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2hsKmaxHfz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode in range(10):\n",
        "    state=env.reset()\n",
        "    for step in range(100):\n",
        "        #env.render()\n",
        "        #print(state)\n",
        "        action=env.action_space.sample()\n",
        "        state,reward,done,info=env.step(action)\n",
        "        if done:\n",
        "            #print(\"Over after \"+str(step)+\"s\")\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxFpY2pYHfz_",
        "colab_type": "text"
      },
      "source": [
        "Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIxOU33YHf0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DNN:\n",
        "    def __init__(self,input_shape):\n",
        "        model=K.models.Sequential()\n",
        "        \n",
        "        model.add(K.layers.Dense(512,input_shape=input_shape,activation='relu',kernel_initializer='he_uniform'))\n",
        "        \n",
        "        model.add(K.layers.Dense(256,activation='relu',kernel_initializer='he_uniform'))\n",
        "    \n",
        "        model.add(K.layers.Dense(64,activation='relu',kernel_initializer='he_uniform'))\n",
        "    \n",
        "        model.add(K.layers.Dense(2,activation='linear',kernel_initializer='he_uniform'))\n",
        "        self.model=model\n",
        "\n",
        "    def get_model(self,adam_lr,adam_lr_decay):\n",
        "        self.model.compile(loss='mse',optimizer=K.optimizers.Adam(lr=adam_lr,decay=adam_lr_decay))\n",
        "        return self.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6deFiMu1Hf0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dnn=DNN((4,)).get_model(0.1,0.01)\n",
        "dnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asWbiR8tHf0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Experience=namedtuple('Experience',('state','action', 'next_state', 'reward','done'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAgG9rgkHf0I",
        "colab_type": "text"
      },
      "source": [
        "Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8-5FH7DHf0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self,size_limit):\n",
        "        self.memory=[]\n",
        "        self.size_limit=size_limit\n",
        "        self.push_count=0\n",
        "\n",
        "    def add(self,Experience):\n",
        "        if len(self.memory)<self.size_limit:\n",
        "            self.memory.append(Experience)\n",
        "        else:\n",
        "            self.memory[self.push_count%self.size_limit]=Experience\n",
        "        self.push_count+=1\n",
        "\n",
        "    def get_sample(self,batch_size):\n",
        "        return random.sample(self.memory,min(batch_size,len(self.memory)))\n",
        "    \n",
        "    def extract(self,samples):\n",
        "        l=[[],[],[],[],[]]\n",
        "        for i in samples:\n",
        "            l[0].append(i[0])\n",
        "            l[1].append(i[1])\n",
        "            l[2].append(i[2])\n",
        "            l[3].append(i[3])\n",
        "            l[4].append(i[4])\n",
        "        return (l[0],l[1],l[2],l[3],l[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJM9gPSBHf0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rm=ReplayMemory(4)\n",
        "rm.add(Experience(2,2,2,1,0))\n",
        "rm.add(Experience(2,2,2,2,1))\n",
        "rm.add(Experience(2,2,2,3,0))\n",
        "rm.add(Experience(2,2,2,4,1))\n",
        "rm.add(Experience(2,2,2,5,0))\n",
        "a=rm.get_sample(2)\n",
        "#print(a)\n",
        "a=rm.extract(a)\n",
        "#print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obT-VsagHf0M",
        "colab_type": "text"
      },
      "source": [
        "Agent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s81L8HCWHf0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self,epsion, epsilon_min,epsilon_decay,gamma,adam_lr,adam_lr_decay,eps_updt_iter):\n",
        "        self.epsilon=epsilon\n",
        "        self.epsilon_min=epsilon_min\n",
        "        self.epsilon_decay=epsilon_decay\n",
        "        self.gamma=gamma\n",
        "        self.eps_updt_iter=eps_updt_iter\n",
        "        self.trainDNN=DNN((4,)).get_model(adam_lr,adam_lr_decay)\n",
        "        self.targetDNN=DNN((4,)).get_model(adam_lr,adam_lr_decay)\n",
        "        self.targetDNN.set_weights(self.trainDNN.get_weights())\n",
        "        \n",
        "    def update(self):\n",
        "        self.targetDNN.set_weights(self.trainDNN.get_weights())\n",
        "        \n",
        "    def select_action(self,state):\n",
        "        if self.epsilon>np.random.random():\n",
        "            return env.action_space.sample()         \n",
        "        else:\n",
        "            return np.argmax(self.trainDNN.predict(state))\n",
        "    \n",
        "    def preprocess(self,state):\n",
        "        return np.reshape(state,[1,4])\n",
        "    \n",
        "    def train(self,rm,batch_size):\n",
        "        if batch_size>len(rm.memory):\n",
        "            return\n",
        "        \n",
        "        sample = rm.get_sample(batch_size)\n",
        "        state, action,next_state, reward, done = rm.extract(sample)\n",
        "        state=np.reshape(np.array(state),(batch_size,4))\n",
        "        next_state=np.reshape(np.array(next_state),(batch_size,4))\n",
        "            \n",
        "        target = self.trainDNN.predict(state)\n",
        "        target_next = self.targetDNN.predict(next_state)\n",
        "        \n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
        "                        \n",
        "        self.trainDNN.fit(state, target, batch_size=batch_size, verbose=0)\n",
        "        \n",
        "        if self.epsilon > self.epsilon_min and len(rm.memory)>self.eps_updt_iter:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        \n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyrHm1UGHf0P",
        "colab_type": "text"
      },
      "source": [
        "Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyC7IBfbHf0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self,agent,rm,env,num_episodes,num_steps,update_iter=5):\n",
        "        self.num_episodes=num_episodes\n",
        "        self.num_steps=num_steps\n",
        "        self.update_iter=update_iter\n",
        "        self.agent=agent\n",
        "        self.env=env\n",
        "        self.rm=rm\n",
        "    \n",
        "    def training(self):\n",
        "        for episode in range(num_episodes):\n",
        "            s=self.env.reset()\n",
        "            state=self.agent.preprocess(s)\n",
        "            done=False\n",
        "            step=0\n",
        "            while not done:\n",
        "                action=self.agent.select_action(state)\n",
        "                next_s,reward,done, info = self.env.step(action)\n",
        "                next_state=self.agent.preprocess(next_s)\n",
        "                self.rm.add(Experience(state,action,next_state,reward,done))\n",
        "                state=next_state\n",
        "                step+=1\n",
        "                self.agent.train(rm,batch_size)\n",
        "                \n",
        "            if episode%self.update_iter==0:\n",
        "                self.agent.update()\n",
        "                print(\"Episode:\"+str(episode),\",Score:\"+str(step))\n",
        "        print(\"Saving trained model\")\n",
        "        #self.agent.trainDNN.save(\"cartpole.h5\")\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        episode_dur=[]\n",
        "        model =K.models.load_model(\"cartpole.h5\")\n",
        "        self.agent.trainDNN.set_weights(model.get_weights())\n",
        "        for episode in range(200):\n",
        "            s=self.env.reset()\n",
        "            state=self.agent.preprocess(s)\n",
        "            done=False\n",
        "            step=0\n",
        "            while not done:\n",
        "                action=np.argmax(self.agent.trainDNN.predict(state))\n",
        "                next_s,reward,done, info = self.env.step(action)\n",
        "                next_state=self.agent.preprocess(next_s)\n",
        "                state=next_state\n",
        "                step+=1\n",
        "                if done:\n",
        "                    print(\"Episode:\"+str(episode),\",Score:\"+str(step))\n",
        "            episode_dur.append(step)\n",
        "        return episode_dur"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRL5oTQWHf0R",
        "colab_type": "text"
      },
      "source": [
        "Defining parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8flh15uiHf0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes=200\n",
        "num_steps=500\n",
        "gamma=0.95\n",
        "\n",
        "epsilon=1\n",
        "epsilon_min=0.001\n",
        "epsilon_decay=0.999\n",
        "eps_update_iter=1000\n",
        "\n",
        "adam_lr=0.001\n",
        "adam_lr_decay=0.001\n",
        "\n",
        "batch_size=64\n",
        "rm_size_limit=1000000\n",
        "\n",
        "trgtDNN_updt_iter=5\n",
        "\n",
        "win_dur=195"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVeQX63WHf0T",
        "colab_type": "text"
      },
      "source": [
        "Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJnCQTVHf0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env=gym.make(\"CartPole-v1\")\n",
        "rm=ReplayMemory(rm_size_limit)\n",
        "\n",
        "agent=Agent(epsilon,epsilon_min,epsilon_decay,gamma,adam_lr,adam_lr_decay,eps_update_iter)\n",
        "\n",
        "Env=Environment(agent,rm,env,num_episodes,num_steps,update_iter=trgtDNN_updt_iter)\n",
        "\n",
        "#Uncomment below line to train\n",
        "#Env.training()\n",
        "\n",
        "#Testing\n",
        "eps_dur=Env.test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTMnh4iHf0V",
        "colab_type": "text"
      },
      "source": [
        "Calculating moving average"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTrUsXuTHf0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Env.agent.trainDNN.save(\"cartpole.h5\")\n",
        "#eps_dur=Env.test()\n",
        "eps=np.linspace(0,len(eps_dur),1)\n",
        "mean=np.zeros(eps.shape)\n",
        "for i in range(len(eps_dur)):\n",
        "    if i<=98:\n",
        "        mean[i]=np.mean(eps_dur[:i+1])\n",
        "    else:\n",
        "        mean[i]=np.mean(eps_dur[i-99:i+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRQlnWbHf0X",
        "colab_type": "text"
      },
      "source": [
        "Moving average plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUtM2tAPHf0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(eps[99:],mean[99:],'b')  \n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Average over last 100 epi\")\n",
        "\n",
        "for i in mean:\n",
        "    if i>win_dur:\n",
        "        print('Solved')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}